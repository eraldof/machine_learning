tunagem <-
all_wf %>%
workflow_map(
seed = 2023,
verbose = TRUE,
resamples = cv,
grid = 50,
metrics = metrica
)
#melhores modelos
modelos_rank <- tunagem %>% rank_results()
melhor_mmo <-
tunagem %>%
extract_workflow_set_result("formula_mmo") %>%
select_best("rmse")
melhor_ridge <-
tunagem %>%
extract_workflow_set_result("formula_ridge") %>%
select_best("rmse")
melhor_lasso <-
tunagem %>%
extract_workflow_set_result("formula_lasso") %>%
select_best("rmse")
melhor_elastic <-
tunagem %>%
extract_workflow_set_result("formula_elastic") %>%
select_best("rmse")
finalizando_mmo <-
tunagem %>%
extract_workflow("formula_mmo") %>%
finalize_workflow(melhor_mmo) %>%
last_fit(split = df)
finalizando_ridge <-
tunagem %>%
extract_workflow("formula_ridge") %>%
finalize_workflow(melhor_ridge) %>%
last_fit(split = df)
finalizando_lasso <-
tunagem %>%
extract_workflow("formula_lasso") %>%
finalize_workflow(melhor_lasso) %>%
last_fit(split = df)
finalizando_elastic <-
tunagem %>%
extract_workflow("formula_elastic") %>%
finalize_workflow(melhor_elastic) %>%
last_fit(split = df)
knitr::kable(data.frame(row.names = c("MMO", "Ridge", "Lasso", "Elastic"),
"RMSE" = as.numeric(c((finalizando_mmo  %>%  collect_metrics())[1,3],
(finalizando_ridge %>% collect_metrics())[1,3],
(finalizando_lasso %>% collect_metrics())[1,3],
(finalizando_elastic %>% collect_metrics())[1,3])))
)
data_temp <- data.frame(ychapeu_mmo = (finalizando_mmo %>% collect_predictions())[2],
ychapeu_ridge = (finalizando_ridge %>% collect_predictions())[2],
ychapeu_lasso = (finalizando_lasso %>% collect_predictions())[2],
ychapeu_elastic = (finalizando_elastic %>% collect_predictions())[2],
y = (finalizando_elastic %>% collect_predictions())[4]
)
colnames(data_temp) <- c("ychapeu_mmo", "ychapeu_ridge", "ychapeu_lasso", "ychapeu_elastic", "y")
data_temp <- as_tibble(data_temp)
g1 <- ggplot(data_temp, aes(x = ychapeu_mmo, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_mmo), colour = "#191970", size = 1) +
labs(title = "Modelo MMO", subtitle = "y_ajustado vs y_real")
g2 <- ggplot(data_temp, aes(x = ychapeu_ridge, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_ridge), colour = "#4169E1", size = 1) +
labs(title = "Modelo ridge", subtitle = "y_ajustado vs y_real")
g3 <- ggplot(data_temp, aes(x = ychapeu_lasso, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_lasso), colour = "orange", size = 1) +
labs(title = "Modelo lasso", subtitle = "y_ajustado vs y_real")
g4 <- ggplot(data_temp, aes(x = ychapeu_elastic, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_elastic), colour = "red", size = 1) +
labs(title = "Modelo elastic", subtitle = "y_ajustado vs y_real")
(g1 + g2) / (g3 + g4)
View(df)
#Transformando qualitativas em numéricas
df <- recipe(charges ~., data = insurance) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep() %>% juice()
knitr::kable(df)
knitr::kable(df %>% head())
#Transformando qualitativas em numéricas
df <- recipe(charges ~., data = insurance) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep() %>% juice()
knitr::kable(df %>% head(15))
#Transformando qualitativas em numéricas
df <- recipe(charges ~., data = insurance) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep() %>% juice()
knitr::kable(df %>% head(10))
#Transformando qualitativas em numéricas
df <- recipe(charges ~., data = insurance) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep() %>% juice()
knitr::kable(df %>% head(10), format = "html")
rm(list = ls())
setwd("C:/Users/erald/Desktop/Faculdade/Aprendizagem de Maquina/machine_learning/")
set.seed(2023)
library(rsample)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(tibble)
library(purrr)
library(patchwork)
tidymodels::tidymodels_prefer()
rm(list = ls())
dados_expectativa_renda <- read.csv("dados_expectativa_renda.csv")
set.seed(2023)
p = 50
dados_exp <- dados_expectativa_renda[,-1]
colnames(dados_exp) <- c("y", "x")
for(i in 1:p){
if(i > 1){
temp <- cbind(temp, temp[,2]^i)
colnames(temp) <- c("y", "x", paste0("x", 2:i))
fit <- lm(y~. , data = temp)
eqm[i] <- mean((temp$y - predict(fit, newdata = temp))^2)
i = i + 1
}
else{
fit <- lm(y~. , data = dados_exp)
eqm <- mean((dados_exp$y - predict(fit, newdata = dados_exp))^2)
temp <- dados_exp
}
}
eqm1 <- data.frame("P" = 1:50, "EQM" = eqm)
g <- ggplot(eqm1, aes(P, EQM)) +
geom_line(linetype = 2, size = 0.4) +
geom_point(size = 2, aes(colour = -EQM)) +
scale_fill_brewer(palette = 1) +
labs(x = "P", y = "EQM",
title = "Avaliação do EQM",
subtitle = "Sem repartir conjunto treino / teste")
g
rm(list = ls())
dados_expectativa_renda <- read.csv("dados_expectativa_renda.csv")
set.seed(2023)
p = 50
dados_exp <- dados_expectativa_renda[,-1]
colnames(dados_exp) <- c("y", "x")
dados_exp <- initial_split(dados_exp, prop = 3/4)
trainer <- training(dados_exp)
tester <- testing(dados_exp)
eqm2 = data.frame(P = 1:50, y = NA)
for(i in 1:p){
if(i > 1){
trainer <- cbind(trainer, trainer[,2]^i)
colnames(trainer) <- c("y", "x", paste0("x", 2:i))
fit <- lm(y~., trainer)
## Avaliando no conjunto de teste
tester <- cbind(tester, tester[,2]^i)
colnames(tester) <- c("y", "x", paste0("x", 2:i))
eqm2[i,2] <- mean((tester[,1] - predict(fit, tester))^2)
}
else{
fit <- lm(y~., trainer)
eqm2[i,2] <- mean((tester[,1] - predict(fit, tester))^2)
}
}
colnames(eqm2) <- c("P", "EQM")
g <- ggplot(eqm2, aes(P, EQM)) +
geom_line(linetype = 2, size = 0.4) +
geom_point(size = 2, aes(colour = -EQM)) +
labs(x = "P", y = "EQM",
title = "Avaliação do EQM",
subtitle = "Repartindo conjunto treino/teste, sem validação cruzada") +
xlim(c(0,18)) + ylim(c(10,130))
g
rm(list = ls())
dados_expectativa_renda <- read.csv("dados_expectativa_renda.csv")
set.seed(2023)
p = 50
dados_exp <- dados_expectativa_renda[,-1]
colnames(dados_exp) <- c("y", "x")
dados_exp <- initial_split(dados_exp, prop = 3/4)
trainer <- training(dados_exp)
tester <- testing(dados_exp)
leave <- loo_cv(trainer)
results1 <- data.frame(P = NA, EQM = NA)
for(b in 1:dim(leave)[1]){
eqm <- NULL
for(i in 1:p){
if(i > 1){
temp_trainer <- leave$splits[[b]] %>% analysis()
temp_tester <- leave$splits[[b]] %>% assessment()
for(j in 2:i){
temp_trainer <- cbind(temp_trainer, temp_trainer$x^j)
temp_tester <- cbind(temp_tester, temp_tester$x^j)
colnames(temp_trainer) <- c("y", "x", paste0("x", 2:j))
colnames(temp_tester) <- c("y", "x", paste0("x", 2:j))
}
fit <- lm(y~., temp_trainer)
eqm[i] <- as.numeric((temp_tester[,1] - predict(fit, temp_tester))^2)
}
else{
temp_trainer <- leave$splits[[b]] %>% analysis()
temp_tester <- leave$splits[[b]] %>% assessment()
fit <- lm(y~., temp_trainer)
eqm[i] <- as.numeric((temp_tester[,1] - predict(fit, newdata = temp_tester))^2)
}
}
results1[b,1] <- which(eqm == min(eqm))[1]
results1[b,2] <- min(eqm)
}
rm(list = ls())
dados_expectativa_renda <- read.csv("dados_expectativa_renda.csv")
set.seed(2023)
p = 50
k = 5
dados_exp <- dados_expectativa_renda[,-1]
colnames(dados_exp) <- c("y", "x")
dados_exp <- initial_split(dados_exp, prop = 3/4)
trainer <- training(dados_exp)
tester <- testing(dados_exp)
vfold <- vfold_cv(trainer, v = 5)
results2 <- data.frame(P = NA, EQM = NA)
for(b in 1:dim(vfold)[1]){
eqm <- NULL
for(i in 1:p){
if(i > 1){
temp_trainer <- vfold$splits[[b]] %>% analysis()
temp_tester <- vfold$splits[[b]] %>% assessment()
for(j in 2:i){
temp_trainer <- cbind(temp_trainer, temp_trainer$x^j)
temp_tester <- cbind(temp_tester, temp_tester$x^j)
colnames(temp_trainer) <- c("y", "x", paste0("x", 2:j))
colnames(temp_tester) <- c("y", "x", paste0("x", 2:j))
}
fit <- lm(y~., temp_trainer)
eqm[i] <- mean((temp_tester[,1] - predict(fit, newdata = temp_tester))^2)
}
else{
temp_trainer <- vfold$splits[[b]] %>% analysis()
temp_tester <- vfold$splits[[b]] %>% assessment()
fit <- lm(y~., temp_trainer)
eqm[i] <- mean((temp_tester[,1] - predict(fit, newdata = temp_tester))^2)
}
}
results2[b,1] <- which(eqm == min(eqm))[1]
results2[b,2] <- min(eqm)
}
rm(list = ls())
wine <- read.csv("winequality-red.csv")
skimr::skim(wine)
rm(list = ls())
setwd("C:/Users/erald/Desktop/Faculdade/Aprendizagem de Maquina/machine_learning/")
wine <- read.csv("winequality-red.csv")
colnames(wine) <- c(paste0("x", 1:11), "y")
#Data split
wine <- initial_split(wine, prop = 3/4)
trainer <- training(wine)
tester <- testing(wine)
#Setting Engine
modelo_mmo <-
linear_reg(penalty = 0, mixture = 0) %>%
set_mode("regression") %>%
set_engine("glmnet")
modelo_ridge <-
linear_reg(penalty = tune::tune(), mixture = 0) %>%
set_mode("regression") %>%
set_engine("glmnet")
modelo_lasso <-
parsnip::linear_reg(penalty = tune::tune(), mixture = 1) %>%
set_mode("regression") %>%
parsnip::set_engine("glmnet")
modelo_elastic <-
parsnip::linear_reg(penalty = tune::tune(), mixture = tune::tune()) %>%
set_mode("regression") %>%
parsnip::set_engine("glmnet")
#workflow
all_wf <-
workflow_set(
preproc = list(y ~ . ),
models = list(mmo = modelo_mmo, ridge = modelo_ridge, lasso = modelo_lasso, elastic = modelo_elastic),
cross = TRUE
)
#cross-validation
set.seed(2023)
cv <- rsample::vfold_cv(trainer, v = 5L)
#metrics
metrica <- yardstick::metric_set(rmse)
#tunning
tunagem <-
all_wf %>%
workflow_map(
seed = 2023,
verbose = TRUE,
resamples = cv,
grid = 50,
metrics = metrica
)
#melhores modelos
modelos_rank <- tunagem %>% rank_results()
melhor_mmo <-
tunagem %>%
extract_workflow_set_result("formula_mmo") %>%
select_best("rmse")
melhor_ridge <-
tunagem %>%
extract_workflow_set_result("formula_ridge") %>%
select_best("rmse")
melhor_lasso <-
tunagem %>%
extract_workflow_set_result("formula_lasso") %>%
select_best("rmse")
melhor_elastic <-
tunagem %>%
extract_workflow_set_result("formula_elastic") %>%
select_best("rmse")
finalizando_mmo <-
tunagem %>%
extract_workflow("formula_mmo") %>%
finalize_workflow(melhor_mmo) %>%
last_fit(split = wine)
finalizando_ridge <-
tunagem %>%
extract_workflow("formula_ridge") %>%
finalize_workflow(melhor_ridge) %>%
last_fit(split = wine)
finalizando_lasso <-
tunagem %>%
extract_workflow("formula_lasso") %>%
finalize_workflow(melhor_lasso) %>%
last_fit(split = wine)
finalizando_elastic <-
tunagem %>%
extract_workflow("formula_elastic") %>%
finalize_workflow(melhor_elastic) %>%
last_fit(split = wine)
knitr::kable(data.frame(row.names = c("MMO", "Ridge", "Lasso", "Elastic"),
"RMSE" = as.numeric(c((finalizando_mmo  %>%  collect_metrics())[1,3],
(finalizando_ridge %>% collect_metrics())[1,3],
(finalizando_lasso %>% collect_metrics())[1,3],
(finalizando_elastic %>% collect_metrics())[1,3])))
)
data_temp <- data.frame(ychapeu_mmo = (finalizando_mmo %>% collect_predictions())[2],
ychapeu_ridge = (finalizando_ridge %>% collect_predictions())[2],
ychapeu_lasso = (finalizando_lasso %>% collect_predictions())[2],
ychapeu_elastic = (finalizando_elastic %>% collect_predictions())[2],
y = (finalizando_elastic %>% collect_predictions())[4]
)
colnames(data_temp) <- c("ychapeu_mmo", "ychapeu_ridge", "ychapeu_lasso", "ychapeu_elastic", "y")
data_temp <- as_tibble(data_temp)
graph <- ggplot(data_temp, aes(x = ychapeu_mmo, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_mmo), colour = "red", size = 1) +
labs(title = "Reta ajustada", subtitle = "y_ajustado vs y_real do modelo MMO")
graph
rm(list = ls())
setwd("C:/Users/erald/Desktop/Faculdade/Aprendizagem de Maquina/machine_learning/")
wine <- read.csv("winequality-red.csv")
colnames(wine) <- c(paste0("x", 1:11), "y")
#Data split
wine <- initial_split(wine, prop = 3/4)
trainer <- training(wine)
tester <- testing(wine)
features <- trainer[,-12] %>% as.matrix()
ytrainer <- trainer[,12] %>% as.matrix()
testfeatures <- tester[,-12] %>% as.matrix()
ytest <- tester[,12] %>% as.matrix()
#MMO
fit <- glmnet(x = features, y = ytrainer, alpha = 0, lambda = 0)
preditos <- predict(fit, newx = testfeatures)
eqm_mmo <- mean((preditos - ytest)^2)
#Ridge
cvridge <- cv.glmnet(x = features, y = ytrainer, alpha = 0, nfolds = 5)
fit2 <- glmnet(x = features, y = ytrainer, alpha = 0)
preditos_ridge <- predict(fit2, s = cvridge$lambda.1se, newx = testfeatures)
eqm_ridge <- mean((preditos_ridge - ytest)^2)
#Lasso
cvlasso <- cv.glmnet(x = features, y = ytrainer, alpha = 1, nfolds = 5)
fit3 <- glmnet(x = features, y = ytrainer, alpha = 1)
preditos_lasso <- predict(fit3, s = cvlasso$lambda.1se, newx = testfeatures)
eqm_lass <- mean((preditos_lasso - ytest)^2)
#Elastic-net
#Precisamos estipular o melhor alpha através de uma validaçao cruzada.
#Utilizando 5-folds.
eqm_elastic <- NULL
alpha_values <- seq(0.01, 0.999, length.out = 500)
for(a in alpha_values){
cvelastic <- cv.glmnet(x = features, y = ytrainer, alpha = a, nfolds = 5)
tempfit <- glmnet(x = features, y = ytrainer, alpha = a)
preditos_elastic <- predict(tempfit, s = cvelastic$lambda.1se, newx = testfeatures)
eqm_elastic <- c(eqm_elastic,
mean((preditos_elastic - ytest)^2))
}
min(eqm_elastic)
alpha_values[which(eqm_elastic == min(eqm_elastic))]
#############
knitr::kable(data.frame(row.names = c("MMO", "Ridge", "Lasso", "Elastic"),
"EQM" = c(eqm_mmo, eqm_ridge, eqm_lass, min(eqm_elastic)),
"RMSE" = sqrt(c(eqm_mmo, eqm_ridge, eqm_lass, min(eqm_elastic)))
)
)
rm(list = ls())
setwd("C:/Users/erald/Desktop/Faculdade/Aprendizagem de Maquina/machine_learning/")
insurance <- read.csv("insurance.csv")
#Transformando qualitativas em numéricas
df <- recipe(charges ~., data = insurance) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
prep() %>% juice()
knitr::kable(df %>% head(10), format = "html")
df <- initial_split(df, prop = 3/4)
trainer <- training(df)
tester <- testing(df)
#Setting Engine
modelo_mmo <-
linear_reg(penalty = 0, mixture = 0) %>%
set_mode("regression") %>%
set_engine("glmnet")
modelo_ridge <-
linear_reg(penalty = tune::tune(), mixture = 0) %>%
set_mode("regression") %>%
set_engine("glmnet")
modelo_lasso <-
parsnip::linear_reg(penalty = tune::tune(), mixture = 1) %>%
set_mode("regression") %>%
parsnip::set_engine("glmnet")
modelo_elastic <-
parsnip::linear_reg(penalty = tune::tune(), mixture = tune::tune()) %>%
set_mode("regression") %>%
parsnip::set_engine("glmnet")
#workflow
all_wf <-
workflow_set(
preproc = list(charges ~ . ),
models = list(mmo = modelo_mmo, ridge = modelo_ridge, lasso = modelo_lasso, elastic = modelo_elastic),
cross = TRUE
)
#cross-validation
set.seed(2023)
cv <- rsample::vfold_cv(trainer, v = 5L)
#metrics
metrica <- yardstick::metric_set(rmse)
#tunning
tunagem <-
all_wf %>%
workflow_map(
seed = 2023,
verbose = TRUE,
resamples = cv,
grid = 50,
metrics = metrica
)
#melhores modelos
modelos_rank <- tunagem %>% rank_results()
melhor_mmo <-
tunagem %>%
extract_workflow_set_result("formula_mmo") %>%
select_best("rmse")
melhor_ridge <-
tunagem %>%
extract_workflow_set_result("formula_ridge") %>%
select_best("rmse")
melhor_lasso <-
tunagem %>%
extract_workflow_set_result("formula_lasso") %>%
select_best("rmse")
melhor_elastic <-
tunagem %>%
extract_workflow_set_result("formula_elastic") %>%
select_best("rmse")
finalizando_mmo <-
tunagem %>%
extract_workflow("formula_mmo") %>%
finalize_workflow(melhor_mmo) %>%
last_fit(split = df)
finalizando_ridge <-
tunagem %>%
extract_workflow("formula_ridge") %>%
finalize_workflow(melhor_ridge) %>%
last_fit(split = df)
finalizando_lasso <-
tunagem %>%
extract_workflow("formula_lasso") %>%
finalize_workflow(melhor_lasso) %>%
last_fit(split = df)
finalizando_elastic <-
tunagem %>%
extract_workflow("formula_elastic") %>%
finalize_workflow(melhor_elastic) %>%
last_fit(split = df)
knitr::kable(data.frame(row.names = c("MMO", "Ridge", "Lasso", "Elastic"),
"RMSE" = as.numeric(c((finalizando_mmo  %>%  collect_metrics())[1,3],
(finalizando_ridge %>% collect_metrics())[1,3],
(finalizando_lasso %>% collect_metrics())[1,3],
(finalizando_elastic %>% collect_metrics())[1,3])))
)
data_temp <- data.frame(ychapeu_mmo = (finalizando_mmo %>% collect_predictions())[2],
ychapeu_ridge = (finalizando_ridge %>% collect_predictions())[2],
ychapeu_lasso = (finalizando_lasso %>% collect_predictions())[2],
ychapeu_elastic = (finalizando_elastic %>% collect_predictions())[2],
y = (finalizando_elastic %>% collect_predictions())[4]
)
colnames(data_temp) <- c("ychapeu_mmo", "ychapeu_ridge", "ychapeu_lasso", "ychapeu_elastic", "y")
data_temp <- as_tibble(data_temp)
g1 <- ggplot(data_temp, aes(x = ychapeu_mmo, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_mmo), colour = "#191970", size = 1) +
labs(title = "Modelo MMO", subtitle = "y_ajustado vs y_real")
g2 <- ggplot(data_temp, aes(x = ychapeu_ridge, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_ridge), colour = "#4169E1", size = 1) +
labs(title = "Modelo ridge", subtitle = "y_ajustado vs y_real")
g3 <- ggplot(data_temp, aes(x = ychapeu_lasso, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_lasso), colour = "orange", size = 1) +
labs(title = "Modelo lasso", subtitle = "y_ajustado vs y_real")
g4 <- ggplot(data_temp, aes(x = ychapeu_elastic, y = y)) +
geom_point() + geom_line(aes(y = ychapeu_elastic), colour = "red", size = 1) +
labs(title = "Modelo elastic", subtitle = "y_ajustado vs y_real")
(g1 + g2) / (g3 + g4)
View(df)
View(insurance)
